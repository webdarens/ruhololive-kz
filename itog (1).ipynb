{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T08:08:32.037463Z",
     "iopub.status.busy": "2025-12-09T08:08:32.037162Z",
     "iopub.status.idle": "2025-12-09T08:08:36.436574Z",
     "shell.execute_reply": "2025-12-09T08:08:36.435819Z",
     "shell.execute_reply.started": "2025-12-09T08:08:32.037441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting implicit\n",
      "  Downloading implicit-0.7.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from implicit) (1.26.4)\n",
      "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.11/dist-packages (from implicit) (1.15.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from implicit) (4.67.1)\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from implicit) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->implicit) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->implicit) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->implicit) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->implicit) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.0->implicit) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.0->implicit) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.0->implicit) (2024.2.0)\n",
      "Downloading implicit-0.7.2-cp311-cp311-manylinux2014_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: implicit\n",
      "Successfully installed implicit-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-09T08:11:20.571269Z",
     "iopub.status.busy": "2025-12-09T08:11:20.570818Z",
     "iopub.status.idle": "2025-12-09T08:14:17.875670Z",
     "shell.execute_reply": "2025-12-09T08:14:17.874878Z",
     "shell.execute_reply.started": "2025-12-09T08:11:20.571242Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "HYBRID RANKING PIPELINE (ALS + CATBOOST + XGBOOST + BLENDING)\n",
      "##########################################\n",
      "train shape      : (269061, 5)\n",
      "books shape      : (55784, 10)\n",
      "users shape      : (7289, 3)\n",
      "candidates shape : (3512, 2)\n",
      "targets shape    : (3512, 1)\n",
      "=== Loading your submission: /kaggle/input/kzntoiii/subik.csv ===\n",
      "Loaded 67450 recommendations from original submission\n",
      "=== target encodings & time decay ===\n",
      "=== ALS  (factors=32, iters=30, reg=0.05) ===\n",
      "interaction_matrix shape: (49944, 7289)  (books x users)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f30479c9ba34a4da4af64098be02e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_factors shape: (7289, 32)\n",
      "item_factors shape: (49944, 32)\n",
      "\n",
      "outer temp split: 2019-06-27 23:17:47.040000\n",
      "past  size: 168,970\n",
      "future size: 100,091\n",
      "=== building training pairs (with negatives) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "users for negatives: 100%|██████████| 5227/5227 [00:08<00:00, 639.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positives: 100,091, negatives: 1,034,401, total: 1,134,492\n",
      "\n",
      "inner temp split: 2018-06-27 12:32:19\n",
      "inner past  size: 135,176\n",
      "inner future size: 33,794\n",
      "=== building training pairs (with negatives) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "users for negatives: 100%|██████████| 2828/2828 [00:04<00:00, 636.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positives: 33,794, negatives: 559,476, total: 593,270\n",
      "=== building features ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n",
      "  return op(a, b)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n",
      "  return op(a, b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (593270, 104)\n",
      "=== building features ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (1134492, 104)\n",
      "=== building features ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n",
      "  return op(a, b)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n",
      "  return op(a, b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (593270, 107)\n",
      "=== building features ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (1134492, 107)\n",
      "=== training CatBoost ranker (N=4) ===\n",
      "num_features: 101; categorical: ['gender']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because NDCG is/are not implemented for GPU\n",
      "Metric NDCG:type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric NDCG:top=20;type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.2964786\tbest: 0.2964786 (0)\ttotal: 139ms\tremaining: 9m 16s\n",
      "200:\ttest: 0.6131389\tbest: 0.6137711 (193)\ttotal: 13.8s\tremaining: 4m 20s\n",
      "bestTest = 0.6137711017\n",
      "bestIteration = 193\n",
      "Shrink model to first 194 iterations.\n",
      "model 1 best_trees: 194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because NDCG is/are not implemented for GPU\n",
      "Metric NDCG:type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric NDCG:top=20;type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.2517542\tbest: 0.2517542 (0)\ttotal: 134ms\tremaining: 8m 56s\n",
      "bestTest = 0.6219762598\n",
      "bestIteration = 15\n",
      "Shrink model to first 16 iterations.\n",
      "model 2 best_trees: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because NDCG is/are not implemented for GPU\n",
      "Metric NDCG:type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric NDCG:top=20;type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.3519172\tbest: 0.3519172 (0)\ttotal: 137ms\tremaining: 9m 7s\n",
      "200:\ttest: 0.6223201\tbest: 0.6227565 (196)\ttotal: 14.2s\tremaining: 4m 28s\n",
      "bestTest = 0.6227564764\n",
      "bestIteration = 196\n",
      "Shrink model to first 197 iterations.\n",
      "model 3 best_trees: 197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because NDCG is/are not implemented for GPU\n",
      "Metric NDCG:type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n",
      "Metric NDCG:top=20;type=Base is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.2284362\tbest: 0.2284362 (0)\ttotal: 136ms\tremaining: 9m 4s\n",
      "200:\ttest: 0.6148826\tbest: 0.6163452 (114)\ttotal: 14.1s\tremaining: 4m 26s\n",
      "bestTest = 0.6163451616\n",
      "bestIteration = 114\n",
      "Shrink model to first 115 iterations.\n",
      "model 4 best_trees: 115\n",
      "=== training XGBoost ranker (N=3) ===\n",
      "XGBoost features: 104\n",
      "XGBoost model 1 trained with 0 iterations\n",
      "XGBoost model 2 trained with 1 iterations\n",
      "XGBoost model 3 trained with 0 iterations\n",
      "\n",
      "=== predicting for candidates ===\n",
      "=== building features ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n",
      "  return op(a, b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (81048, 103)\n",
      "=== building features ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n",
      "  return op(a, b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (81048, 106)\n",
      "=== Blending predictions: CatBoost=0.7, XGBoost=0.2, Original=0.1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diversifying: 100%|██████████| 3512/3512 [00:04<00:00, 775.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== building submission ===\n",
      "avg recs per user: 19.21\n",
      "share users with 0 recs: 0.0000\n",
      "\n",
      "submission saved to: /kaggle/working/subik.csv\n",
      "   user_id                                       book_id_list\n",
      "0      210  3015694,2225251,1673950,3988468,971259,1281035...\n",
      "1     1380  482934,2548861,2290484,2379664,1326209,2186305...\n",
      "2     2050  2254200,2053462,317849,822326,18790,2575827,86...\n",
      "3     2740  181062,162418,1737221,2107128,112023,1553798,1...\n",
      "4     4621  3015694,28901,2576738,2225251,28638,28642,2191...\n",
      "##########################################\n",
      "pipeline finished\n",
      "##########################################\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD   \n",
    "import implicit\n",
    "from catboost import CatBoostRanker, Pool\n",
    "from catboost.utils import get_gpu_device_count\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "data_root = Path(\"/kaggle/input/kzntoiii/\")\n",
    "work_root = Path(\"/kaggle/working\")\n",
    "submission_path = data_root / \"whyso.csv\" \n",
    "\n",
    "seed = 42\n",
    "val_split_q = 0.628\n",
    "inner_train_q = 0.8\n",
    "neg_popular_count = 176\n",
    "neg_hard_per_pos = 2\n",
    "neg_random_count = 20\n",
    "popular_pool_size = 7231\n",
    "als_factors = 32\n",
    "als_iterations = 30\n",
    "als_reg = 0.05\n",
    "ensemble_n = 4\n",
    "\n",
    "\n",
    "blend_weight_catboost = 0.70  \n",
    "blend_weight_xgboost = 0.20   #Вес XGBoost\n",
    "blend_weight_original = 0.10  #Вес csv\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "def load_submission():\n",
    "        if submission_path.exists():\n",
    "        print(f\"=== Loading your submission: {submission_path} ===\")\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        sub_exp = sub.copy()\n",
    "        sub_exp[\"book_id_list\"] = sub_exp[\"book_id_list\"].astype(str).str.replace(r'\\.0', '', regex=True)\n",
    "        sub_exp[\"book_id_list\"] = sub_exp[\"book_id_list\"].str.split(\",\")\n",
    "        sub_exp = sub_exp.explode(\"book_id_list\").rename(columns={\"book_id_list\": \"book_id\"})\n",
    "        sub_exp[\"book_id\"] = pd.to_numeric(sub_exp[\"book_id\"], errors='coerce').fillna(0).astype(int)\n",
    "        #Удаляем нулевые book_id\n",
    "        sub_exp = sub_exp[sub_exp[\"book_id\"] > 0]\n",
    "        #Создаем ранги (чем выше в списке, тем лучше)\n",
    "        sub_exp[\"rank\"] = sub_exp.groupby(\"user_id\").cumcount()\n",
    "        sub_exp[\"original_score\"] = 1.0 / (sub_exp[\"rank\"] + 1.0)\n",
    "        print(f\"Loaded {len(sub_exp)} recommendations from original submission\")\n",
    "        return sub_exp\n",
    "    else:\n",
    "        print(\"WARNING: submission.csv not found, will use only new models\")\n",
    "        return None\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_csv(data_root / \"train.csv\", parse_dates=[\"timestamp\"])\n",
    "    candidates = pd.read_csv(data_root / \"candidates.csv\")\n",
    "    books = pd.read_csv(data_root / \"books.csv\")\n",
    "    users = pd.read_csv(data_root / \"users.csv\")\n",
    "    book_genres = pd.read_csv(data_root / \"book_genres.csv\")\n",
    "    targets = pd.read_csv(data_root / \"targets.csv\")\n",
    "\n",
    "    g_map = book_genres.groupby(\"book_id\", as_index=False)[\"genre_id\"].first()\n",
    "    books = books.merge(g_map, on=\"book_id\", how=\"left\")\n",
    "    books = books.drop_duplicates(subset=[\"book_id\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    if \"title\" in books.columns:\n",
    "        books[\"title_length\"] = books[\"title\"].fillna(\"\").str.len()\n",
    "    if \"description\" in books.columns:\n",
    "        books[\"description_length\"] = books[\"description\"].fillna(\"\").str.len()\n",
    "\n",
    "    print(f\"train shape      : {train.shape}\")\n",
    "    print(f\"books shape      : {books.shape}\")\n",
    "    print(f\"users shape      : {users.shape}\")\n",
    "    print(f\"candidates shape : {candidates.shape}\")\n",
    "    print(f\"targets shape    : {targets.shape}\")\n",
    "    return train, candidates, books, users, book_genres, targets\n",
    "\n",
    "def als_fit(train_df, n_factors=32, n_iter=30, reg=0.05):\n",
    "    print(f\"=== ALS  (factors={n_factors}, iters={n_iter}, reg={reg}) ===\")\n",
    "    t = train_df[[\"user_id\", \"book_id\", \"has_read\"]].copy()\n",
    "    t[\"weight\"] = t[\"has_read\"].fillna(1).astype(float)\n",
    "\n",
    "    all_users = t[\"user_id\"].unique()\n",
    "    all_books = t[\"book_id\"].unique()\n",
    "    n_users = len(all_users)\n",
    "    n_items = len(all_books)\n",
    "\n",
    "    u2i = {u: i for i, u in enumerate(all_users)}\n",
    "    b2i = {b: i for i, b in enumerate(all_books)}\n",
    "\n",
    "    r = t[\"book_id\"].map(b2i).to_numpy()\n",
    "    c = t[\"user_id\"].map(u2i).to_numpy()\n",
    "    v = t[\"weight\"].to_numpy()\n",
    "\n",
    "    mat = sparse.csr_matrix((v, (r, c)), shape=(n_items, n_users))\n",
    "    print(f\"interaction_matrix shape: {mat.shape}  (books x users)\")\n",
    "\n",
    "    model = implicit.als.AlternatingLeastSquares(\n",
    "        factors=n_factors,\n",
    "        regularization=reg,\n",
    "        iterations=n_iter,\n",
    "        use_gpu=False\n",
    "    )\n",
    "    model.fit(mat)\n",
    "\n",
    "    it_f = model.item_factors\n",
    "    us_f = model.user_factors\n",
    "    if it_f.shape[0] != n_items:\n",
    "        it_f, us_f = us_f, it_f\n",
    "\n",
    "    print(f\"user_factors shape: {us_f.shape}\")\n",
    "    print(f\"item_factors shape: {it_f.shape}\")\n",
    "\n",
    "    u_tab = pd.DataFrame(us_f)\n",
    "    u_tab.insert(0, \"user_id\", all_users)\n",
    "    u_tab.columns = [\"user_id\"] + [f\"user_emb_{i}\" for i in range(us_f.shape[1])]\n",
    "\n",
    "    b_tab = pd.DataFrame(it_f)\n",
    "    b_tab.insert(0, \"book_id\", all_books)\n",
    "    b_tab.columns = [\"book_id\"] + [f\"book_emb_{i}\" for i in range(it_f.shape[1])]\n",
    "    return u_tab, b_tab\n",
    "\n",
    "def mk_te(train_df, books_df, t_max, smoothing=10, decay_days=30):\n",
    "    print(\"=== target encodings & time decay ===\")\n",
    "    m = train_df.merge(books_df[[\"book_id\", \"author_id\", \"genre_id\"]], on=\"book_id\", how=\"left\")\n",
    "    prior = m[\"has_read\"].mean()\n",
    "\n",
    "    a = m.groupby(\"author_id\")[\"has_read\"].agg([\"mean\", \"count\"]).reset_index()\n",
    "    a[\"author_te\"] = (a[\"mean\"] * a[\"count\"] + prior * smoothing) / (a[\"count\"] + smoothing)\n",
    "    author_te = a[[\"author_id\", \"author_te\"]]\n",
    "\n",
    "    g = m.groupby(\"genre_id\")[\"has_read\"].agg([\"mean\", \"count\"]).reset_index()\n",
    "    g[\"genre_te\"] = (g[\"mean\"] * g[\"count\"] + prior * smoothing) / (g[\"count\"] + smoothing)\n",
    "    genre_te = g[[\"genre_id\", \"genre_te\"]]\n",
    "\n",
    "    ref = pd.to_datetime(t_max)\n",
    "    tt = train_df[[\"book_id\", \"timestamp\"]].copy()\n",
    "    tt[\"days_ago\"] = (ref - pd.to_datetime(tt[\"timestamp\"])).dt.total_seconds() / 86400.0\n",
    "    tt[\"decay_weight\"] = np.exp(-tt[\"days_ago\"] / decay_days)\n",
    "\n",
    "    book_decay = tt.groupby(\"book_id\")[\"decay_weight\"].sum().reset_index()\n",
    "    book_decay = book_decay.rename(columns={\"decay_weight\": \"book_time_decay\"})\n",
    "    return author_te, genre_te, book_decay\n",
    "\n",
    "def mk_pairs(pos_df, hist_df, books_df):\n",
    "    print(\"=== building training pairs (with negatives) ===\")\n",
    "    p = pos_df[[\"user_id\", \"book_id\", \"has_read\"]].copy()\n",
    "    p[\"relevance\"] = p[\"has_read\"].map({1: 2, 0: 1}).astype(int)\n",
    "\n",
    "    user_seen = hist_df.groupby(\"user_id\")[\"book_id\"].agg(set).to_dict()\n",
    "    popular_books = hist_df[\"book_id\"].value_counts().head(popular_pool_size).index.tolist()\n",
    "    all_books = books_df[\"book_id\"].unique()\n",
    "\n",
    "    m_a = books_df.set_index(\"book_id\")[\"author_id\"].to_dict() if \"author_id\" in books_df.columns else {}\n",
    "    m_g = books_df.set_index(\"book_id\")[\"genre_id\"].to_dict() if \"genre_id\" in books_df.columns else {}\n",
    "\n",
    "    a_books = defaultdict(list)\n",
    "    g_books = defaultdict(list)\n",
    "    for _, row in books_df.iterrows():\n",
    "        b_id = row[\"book_id\"]\n",
    "        a_id = row.get(\"author_id\", np.nan)\n",
    "        g_id = row.get(\"genre_id\", np.nan)\n",
    "        if pd.notna(a_id):\n",
    "            a_books[a_id].append(b_id)\n",
    "        if pd.notna(g_id):\n",
    "            g_books[g_id].append(b_id)\n",
    "\n",
    "    negs = []\n",
    "    for uid in tqdm(p[\"user_id\"].unique(), desc=\"users for negatives\"):\n",
    "        seen = user_seen.get(uid, set())\n",
    "        u_pos = p.loc[p[\"user_id\"] == uid, \"book_id\"].unique()\n",
    "        sampled = set()\n",
    "\n",
    "        h_cnt = 0\n",
    "        for pb in u_pos[:5]:\n",
    "            a = m_a.get(pb, -1)\n",
    "            g = m_g.get(pb, -1)\n",
    "            cand = a_books.get(a, []) + g_books.get(g, [])\n",
    "            random.shuffle(cand)\n",
    "            for cb in cand:\n",
    "                if cb not in seen and cb not in sampled:\n",
    "                    negs.append((uid, cb, 0))\n",
    "                    sampled.add(cb)\n",
    "                    h_cnt += 1\n",
    "                    break\n",
    "            if h_cnt >= neg_hard_per_pos:\n",
    "                break\n",
    "\n",
    "        pop_cnt = 0\n",
    "        for pb in popular_books:\n",
    "            if pb not in seen and pb not in sampled:\n",
    "                negs.append((uid, pb, 0))\n",
    "                sampled.add(pb)\n",
    "                pop_cnt += 1\n",
    "            if pop_cnt >= neg_popular_count:\n",
    "                break\n",
    "\n",
    "        r_cnt = 0\n",
    "        while r_cnt < neg_random_count:\n",
    "            rb = int(np.random.choice(all_books))\n",
    "            if rb not in seen and rb not in sampled:\n",
    "                negs.append((uid, rb, 0))\n",
    "                sampled.add(rb)\n",
    "                r_cnt += 1\n",
    "\n",
    "    neg_tab = pd.DataFrame(negs, columns=[\"user_id\", \"book_id\", \"relevance\"])\n",
    "    mix = pd.concat([p[[\"user_id\", \"book_id\", \"relevance\"]], neg_tab], ignore_index=True)\n",
    "    print(f\"positives: {len(p):,}, negatives: {len(neg_tab):,}, total: {len(mix):,}\")\n",
    "    mix = mix.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return mix\n",
    "\n",
    "def mk_feat(data_df, hist_df, books_df, users_df, t_ref, user_emb, book_emb, \n",
    "            author_te, genre_te, book_decay, for_xgb=False):\n",
    "    print(\"=== building features ===\")\n",
    "    m = data_df.copy()\n",
    "\n",
    "    u = hist_df.groupby(\"user_id\").agg(\n",
    "        user_interaction_cnt=(\"book_id\", \"count\"),\n",
    "        user_read_cnt=(\"has_read\", \"sum\"),\n",
    "        user_avg_rating=(\"rating\", \"mean\"),\n",
    "        user_rating_std=(\"rating\", \"std\"),\n",
    "        user_last_ts=(\"timestamp\", \"max\"),\n",
    "    ).reset_index()\n",
    "    u[\"user_read_rate\"] = u[\"user_read_cnt\"] / (u[\"user_interaction_cnt\"] + 1e-6)\n",
    "\n",
    "    b = hist_df.groupby(\"book_id\").agg(\n",
    "        book_interaction_cnt=(\"user_id\", \"count\"),\n",
    "        book_read_cnt=(\"has_read\", \"sum\"),\n",
    "        book_avg_rating=(\"rating\", \"mean\"),\n",
    "        book_unique_users=(\"user_id\", \"nunique\"),\n",
    "    ).reset_index()\n",
    "    b[\"book_read_rate\"] = b[\"book_read_cnt\"] / (b[\"book_interaction_cnt\"] + 1e-6)\n",
    "    b[\"book_pop_rank\"] = b[\"book_interaction_cnt\"].rank(method=\"dense\", ascending=False)\n",
    "\n",
    "    m = m.merge(b, on=\"book_id\", how=\"left\")\n",
    "    m = m.merge(u, on=\"user_id\", how=\"left\")\n",
    "    m = m.merge(users_df, on=\"user_id\", how=\"left\")\n",
    "\n",
    "    b_num = books_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in [\"book_id\", \"author_id\", \"genre_id\"]:\n",
    "        if col in books_df.columns and col not in b_num:\n",
    "            b_num.append(col)\n",
    "    m = m.merge(books_df[b_num], on=\"book_id\", how=\"left\")\n",
    "\n",
    "    m = m.merge(author_te, on=\"author_id\", how=\"left\")\n",
    "    if \"author_te\" in m.columns:\n",
    "        m[\"author_te\"] = m[\"author_te\"].fillna(author_te[\"author_te\"].mean())\n",
    "    m = m.merge(genre_te, on=\"genre_id\", how=\"left\")\n",
    "    if \"genre_te\" in m.columns:\n",
    "        m[\"genre_te\"] = m[\"genre_te\"].fillna(genre_te[\"genre_te\"].mean())\n",
    "\n",
    "    m = m.merge(book_decay, on=\"book_id\", how=\"left\")\n",
    "    m[\"book_time_decay\"] = m[\"book_time_decay\"].fillna(0.0)\n",
    "\n",
    "    if \"author_id\" in books_df.columns:\n",
    "        h_ext = hist_df.merge(books_df[[\"book_id\", \"author_id\", \"genre_id\"]], on=\"book_id\", how=\"left\")\n",
    "\n",
    "        ua = h_ext.groupby([\"user_id\", \"author_id\"]).agg(\n",
    "            user_author_cnt=(\"book_id\", \"count\"),\n",
    "            user_author_reads=(\"has_read\", \"sum\"),\n",
    "        ).reset_index()\n",
    "        ua[\"user_author_rate\"] = ua[\"user_author_reads\"] / (ua[\"user_author_cnt\"] + 1e-6)\n",
    "        m = m.merge(ua, on=[\"user_id\", \"author_id\"], how=\"left\")\n",
    "\n",
    "        ug = h_ext.groupby([\"user_id\", \"genre_id\"]).agg(\n",
    "            user_genre_cnt=(\"book_id\", \"count\"),\n",
    "            user_genre_reads=(\"has_read\", \"sum\"),\n",
    "        ).reset_index()\n",
    "        ug[\"user_genre_rate\"] = ug[\"user_genre_reads\"] / (ug[\"user_genre_cnt\"] + 1e-6)\n",
    "        m = m.merge(ug, on=[\"user_id\", \"genre_id\"], how=\"left\")\n",
    "\n",
    "    for col in [\"user_author_cnt\", \"user_author_reads\", \"user_author_rate\",\n",
    "                \"user_genre_cnt\", \"user_genre_reads\", \"user_genre_rate\"]:\n",
    "        if col not in m.columns:\n",
    "            m[col] = 0.0\n",
    "        m[col] = m[col].fillna(0.0)\n",
    "\n",
    "    ref_ts = pd.to_datetime(t_ref)\n",
    "    m[\"days_since_active\"] = (ref_ts - pd.to_datetime(m[\"user_last_ts\"])).dt.total_seconds() / 86400.0\n",
    "    mx = m[\"days_since_active\"].max(skipna=True)\n",
    "    if pd.isna(mx):\n",
    "        mx = 365.0\n",
    "    m[\"days_since_active\"] = m[\"days_since_active\"].fillna(mx + 1.0)\n",
    "    m = m.drop(columns=[\"user_last_ts\"], errors=\"ignore\")\n",
    "\n",
    "    ref_year = ref_ts.year\n",
    "    if \"publication_year\" in m.columns:\n",
    "        m[\"book_age\"] = ref_year - m[\"publication_year\"]\n",
    "        m[\"book_age\"] = m[\"book_age\"].fillna(m[\"book_age\"].median())\n",
    "        m[\"book_age\"] = m[\"book_age\"].clip(0, 200)\n",
    "\n",
    "    m = m.merge(user_emb, on=\"user_id\", how=\"left\")\n",
    "    m = m.merge(book_emb, on=\"book_id\", how=\"left\")\n",
    "\n",
    "    #Базовые кросс-фичи для обеих моделей\n",
    "    m[\"affinity\"] = m[\"user_read_rate\"] * m[\"book_read_rate\"]\n",
    "    m[\"rating_diff\"] = m[\"user_avg_rating\"] - m[\"book_avg_rating\"]\n",
    "    m[\"popularity_log\"] = np.log1p(m[\"book_interaction_cnt\"])\n",
    "    m[\"activity_log\"] = np.log1p(m[\"user_interaction_cnt\"])\n",
    "    m[\"is_active_user\"] = (m[\"user_interaction_cnt\"] > 10).astype(int)\n",
    "    m[\"is_popular_book\"] = (m[\"book_pop_rank\"] < m[\"book_pop_rank\"].median()).astype(int)\n",
    "\n",
    "    #Дополнительные кросс-фичи только для XGBoost\n",
    "    if for_xgb:\n",
    "        #Кросс-фичи\n",
    "        if \"user_age\" in m.columns and \"book_age\" in m.columns:\n",
    "            m[\"age_interaction\"] = m[\"user_age\"] * m[\"book_age\"]\n",
    "        m[\"pop_activity_interaction\"] = m[\"popularity_log\"] * m[\"activity_log\"]\n",
    "        m[\"author_user_affinity\"] = m[\"author_te\"] * m[\"user_read_rate\"]\n",
    "        m[\"genre_user_affinity\"] = m[\"genre_te\"] * m[\"user_read_rate\"]\n",
    "        \n",
    "    if \"gender\" in m.columns:\n",
    "        gmap = {\"M\": 1, \"F\": 0, \"male\": 1, \"female\": 0}\n",
    "        m[\"gender\"] = m[\"gender\"].map(gmap).fillna(-1).astype(int)\n",
    "\n",
    "    obj_cols = m.select_dtypes(include=[object]).columns.tolist()\n",
    "    drop_cols = [c for c in obj_cols if c not in [\"user_id\", \"book_id\"]]\n",
    "    m = m.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    num_cols = m.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in num_cols:\n",
    "        if col not in [\"user_id\", \"book_id\", \"relevance\"]:\n",
    "            m[col] = m[col].fillna(0)\n",
    "\n",
    "    print(f\"features shape: {m.shape}\")\n",
    "    return m\n",
    "\n",
    "def fit_cat(x_train, y_train, x_val, y_val, n_models=4):\n",
    "    print(f\"=== training CatBoost ranker (N={n_models}) ===\")\n",
    "    skip = [\"user_id\", \"book_id\", \"relevance\"]\n",
    "    feat_cols = [\n",
    "        c for c in x_train.columns\n",
    "        if (c not in skip)\n",
    "        and (x_train[c].dtype in [np.float64, np.float32, np.int64, np.int32])\n",
    "    ]\n",
    "\n",
    "    cat_cols = [\"gender\"] if \"gender\" in feat_cols else []\n",
    "    print(f\"num_features: {len(feat_cols)}; categorical: {cat_cols}\")\n",
    "\n",
    "    def sort_grp(x, y):\n",
    "        z = x.copy()\n",
    "        z[\"_y\"] = y.values\n",
    "        z = z.sort_values([\"user_id\", \"_y\"], ascending=[True, False]).reset_index(drop=True)\n",
    "        out_y = z[\"_y\"].copy()\n",
    "        z = z.drop(columns=[\"_y\"])\n",
    "        return z, out_y\n",
    "\n",
    "    x_tr, y_tr = sort_grp(x_train, y_train)\n",
    "    x_vl, y_vl = sort_grp(x_val, y_val)\n",
    "\n",
    "    for c in cat_cols:\n",
    "        x_tr[c] = x_tr[c].astype(\"category\")\n",
    "        x_vl[c] = x_vl[c].astype(\"category\")\n",
    "\n",
    "    train_pool = Pool(x_tr[feat_cols], y_tr, group_id=x_tr[\"user_id\"].astype(str), cat_features=cat_cols or None)\n",
    "    val_pool = Pool(x_vl[feat_cols], y_vl, group_id=x_vl[\"user_id\"].astype(str), cat_features=cat_cols or None)\n",
    "\n",
    "    models = []\n",
    "    for i in range(n_models):\n",
    "        params = {\n",
    "            \"iterations\": 4000,\n",
    "            \"depth\": 7,\n",
    "            \"loss_function\": \"YetiRank\",\n",
    "            \"early_stopping_rounds\": 100,\n",
    "            \"l2_leaf_reg\": 2.0 + i * 0.5,\n",
    "            \"learning_rate\": 0.04,\n",
    "            \"border_count\": 254,\n",
    "            \"eval_metric\": \"NDCG:top=20\",\n",
    "            \"random_seed\": seed + i * 13,\n",
    "            \"bagging_temperature\": 0.8 + i * 0.1,\n",
    "            \"verbose\": 200,\n",
    "            \"task_type\": \"GPU\"\n",
    "        }\n",
    " \n",
    "        model = CatBoostRanker(**params)\n",
    "        model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "        print(f\"model {i + 1} best_trees: {model.tree_count_}\")\n",
    "        models.append(model)\n",
    "        gc.collect()\n",
    "\n",
    "    return models, feat_cols, cat_cols\n",
    "\n",
    "def fit_xgb(x_train, y_train, x_val, y_val, n_models=3):\n",
    "\n",
    "    print(f\"=== training XGBoost ranker (N={n_models}) ===\")\n",
    "    skip = [\"user_id\", \"book_id\", \"relevance\"]\n",
    "    feat_cols = [\n",
    "        c for c in x_train.columns\n",
    "        if (c not in skip)\n",
    "        and (x_train[c].dtype in [np.float64, np.float32, np.int64, np.int32])\n",
    "    ]\n",
    "    \n",
    "    print(f\"XGBoost features: {len(feat_cols)}\")\n",
    "    \n",
    "    def sort_grp(x, y):\n",
    "        z = x.copy()\n",
    "        z[\"_y\"] = y.values\n",
    "        z = z.sort_values([\"user_id\", \"_y\"], ascending=[True, False]).reset_index(drop=True)\n",
    "        out_y = z[\"_y\"].copy()\n",
    "        z = z.drop(columns=[\"_y\"])\n",
    "        return z, out_y\n",
    "    \n",
    "    x_tr, y_tr = sort_grp(x_train, y_train)\n",
    "    x_vl, y_vl = sort_grp(x_val, y_val)\n",
    "    \n",
    "    models = []\n",
    "    for i in range(n_models):\n",
    "        params = {\n",
    "            \"objective\": \"rank:ndcg\",\n",
    "            \"eval_metric\": \"ndcg@20\",\n",
    "            \"tree_method\": \"gpu_hist\",\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": 6,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"alpha\": 0.1,\n",
    "            \"random_state\": seed + i * 17,\n",
    "            \"n_estimators\": 2000,\n",
    "            \"early_stopping_rounds\": 50,\n",
    "            \"verbosity\": 0\n",
    "        }\n",
    "        \n",
    "        model = xgb.XGBRanker(**params)\n",
    "        model.fit(\n",
    "            x_tr[feat_cols], y_tr,\n",
    "            group=x_tr.groupby(\"user_id\").size().values,\n",
    "            eval_set=[(x_vl[feat_cols], y_vl)],\n",
    "            eval_group=[x_vl.groupby(\"user_id\").size().values],\n",
    "            verbose=False\n",
    "        )\n",
    "        print(f\"XGBoost model {i+1} trained with {model.best_iteration} iterations\")\n",
    "        models.append(model)\n",
    "        gc.collect()\n",
    "    \n",
    "    return models, feat_cols\n",
    "\n",
    "def pred_cat(models, x_df, feat_cols, cat_cols):\n",
    "    x_loc = x_df[feat_cols].copy()\n",
    "    x_loc = x_loc.fillna(0)\n",
    "\n",
    "    for c in cat_cols:\n",
    "        if c in x_loc.columns:\n",
    "            x_loc[c] = x_loc[c].astype(\"category\")\n",
    "\n",
    "    preds = np.zeros(len(x_loc), dtype=float)\n",
    "    for m in models:\n",
    "        pool = Pool(x_loc, group_id=x_df[\"user_id\"].astype(str), cat_features=cat_cols or None)\n",
    "        preds += m.predict(pool)\n",
    "\n",
    "    preds /= max(1, len(models))\n",
    "    return preds\n",
    "\n",
    "def pred_xgb(models, x_df, feat_cols):\n",
    "    \"\"\"Предсказания XGBoost\"\"\"\n",
    "    x_loc = x_df[feat_cols].copy().fillna(0)\n",
    "    preds = np.zeros(len(x_loc), dtype=float)\n",
    "    for m in models:\n",
    "        preds += m.predict(x_loc)\n",
    "    preds /= max(1, len(models))\n",
    "    return preds\n",
    "\n",
    "def blend_predictions(cat_scores, xgb_scores, original_df, test_pairs, \n",
    "                     w_cat=0.70, w_xgb=0.20, w_orig=0.10):\n",
    "    print(f\"=== predictions: CatBoost={w_cat}, XGBoost={w_xgb}, Original={w_orig} ===\")\n",
    "    \n",
    "    #Нормализуем скоры\n",
    "    test_pairs[\"cat_score\"] = cat_scores\n",
    "    test_pairs[\"xgb_score\"] = xgb_scores\n",
    "    \n",
    "    #Нормализация по пользователю\n",
    "    def normalize_scores(df, score_col):\n",
    "        df[f\"{score_col}_norm\"] = df.groupby(\"user_id\")[score_col].transform(\n",
    "            lambda x: (x - x.mean()) / (x.std() + 1e-6) if x.std() > 0 else 0\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    test_pairs = normalize_scores(test_pairs, \"cat_score\")\n",
    "    test_pairs = normalize_scores(test_pairs, \"xgb_score\")\n",
    "    \n",
    "\n",
    "    if original_df is not None:\n",
    "        test_pairs = test_pairs.merge(\n",
    "            original_df[[\"user_id\", \"book_id\", \"original_score\"]], \n",
    "            on=[\"user_id\", \"book_id\"], how=\"left\"\n",
    "        )\n",
    "        test_pairs[\"original_score\"] = test_pairs[\"original_score\"].fillna(0)\n",
    "        \n",
    "\n",
    "        test_pairs = normalize_scores(test_pairs, \"original_score\")\n",
    "    else:\n",
    "        test_pairs[\"original_score_norm\"] = 0\n",
    "    \n",
    "\n",
    "    test_pairs[\"score\"] = (\n",
    "        w_cat * test_pairs[\"cat_score_norm\"] +\n",
    "        w_xgb * test_pairs[\"xgb_score_norm\"] +\n",
    "        w_orig * test_pairs[\"original_score_norm\"]\n",
    "    )\n",
    "    \n",
    "    return test_pairs\n",
    "\n",
    "def main():\n",
    "    print(\"#\" * 42)\n",
    "    print(\"HYBRID RANKING PIPELINE (ALS + CATBOOST + XGBOOST + BLENDING)\")\n",
    "    print(\"#\" * 42)\n",
    "\n",
    "\n",
    "    train, candidates, books, users, book_genres, targets = read_data()\n",
    "    t_max = train[\"timestamp\"].max()\n",
    "\n",
    "\n",
    "    original_submission = load_submission()\n",
    "\n",
    "    #Подготовка фичей\n",
    "    author_te, genre_te, book_decay = mk_te(train, books, t_max)\n",
    "    user_emb, book_emb = als_fit(train, n_factors=als_factors, n_iter=als_iterations, reg=als_reg)\n",
    "\n",
    "    #Внешний сплит\n",
    "    t_split = train[\"timestamp\"].quantile(val_split_q)\n",
    "    past = train[train[\"timestamp\"] < t_split].copy()\n",
    "    future = train[train[\"timestamp\"] >= t_split].copy()\n",
    "\n",
    "    print(f\"\\nouter temp split: {t_split}\")\n",
    "    print(f\"past  size: {len(past):,}\")\n",
    "    print(f\"future size: {len(future):,}\")\n",
    "\n",
    "    val_pairs = mk_pairs(future, train, books)\n",
    "\n",
    "    #Внутренний сплит\n",
    "    t_inner = past[\"timestamp\"].quantile(inner_train_q)\n",
    "    inner_past = past[past[\"timestamp\"] < t_inner].copy()\n",
    "    inner_future = past[past[\"timestamp\"] >= t_inner].copy()\n",
    "\n",
    "    print(f\"\\ninner temp split: {t_inner}\")\n",
    "    print(f\"inner past  size: {len(inner_past):,}\")\n",
    "    print(f\"inner future size: {len(inner_future):,}\")\n",
    "\n",
    "    train_pairs = mk_pairs(inner_future, past, books)\n",
    "\n",
    "    #Фичи для CatBoost\n",
    "    x_train_cat = mk_feat(train_pairs, inner_past, books, users, t_inner,\n",
    "                         user_emb, book_emb, author_te, genre_te, book_decay, for_xgb=False)\n",
    "    y_train = train_pairs[\"relevance\"]\n",
    "\n",
    "    x_val_cat = mk_feat(val_pairs, past, books, users, t_split,\n",
    "                       user_emb, book_emb, author_te, genre_te, book_decay, for_xgb=False)\n",
    "    y_val = val_pairs[\"relevance\"]\n",
    "\n",
    "    #Фичи для XGBoost(с дополнительными)\n",
    "    x_train_xgb = mk_feat(train_pairs, inner_past, books, users, t_inner,\n",
    "                         user_emb, book_emb, author_te, genre_te, book_decay, for_xgb=True)\n",
    "    x_val_xgb = mk_feat(val_pairs, past, books, users, t_split,\n",
    "                       user_emb, book_emb, author_te, genre_te, book_decay, for_xgb=True)\n",
    "\n",
    "    #Обучение моделей\n",
    "    cat_models, cat_feat_cols, cat_cols = fit_cat(x_train_cat, y_train, x_val_cat, y_val, n_models=ensemble_n)\n",
    "    xgb_models, xgb_feat_cols = fit_xgb(x_train_xgb, y_train, x_val_xgb, y_val, n_models=3)\n",
    "\n",
    " \n",
    "    print(\"\\n=== predicting for candidates ===\")\n",
    "    cand = candidates.copy()\n",
    "    cand[\"book_id_list\"] = cand[\"book_id_list\"].astype(str).str.split(\",\")\n",
    "    cand_exp = cand.explode(\"book_id_list\").rename(columns={\"book_id_list\": \"book_id\"})\n",
    "    cand_exp[\"book_id\"] = cand_exp[\"book_id\"].astype(int)\n",
    "\n",
    "    test_pairs = cand_exp[[\"user_id\", \"book_id\"]].copy()\n",
    "    \n",
    "    #CatBoost\n",
    "    x_test_cat = mk_feat(test_pairs, train, books, users, t_max,\n",
    "                        user_emb, book_emb, author_te, genre_te, book_decay, for_xgb=False)\n",
    "    \n",
    "\n",
    "    for f in cat_feat_cols:\n",
    "        if f not in x_test_cat.columns:\n",
    "            x_test_cat[f] = 0.0\n",
    "    \n",
    "    cat_scores = pred_cat(cat_models, x_test_cat, cat_feat_cols, cat_cols)\n",
    "    \n",
    "    #XGBoost\n",
    "    x_test_xgb = mk_feat(test_pairs, train, books, users, t_max,\n",
    "                        user_emb, book_emb, author_te, genre_te, book_decay, for_xgb=True)\n",
    "    \n",
    "\n",
    "    for f in xgb_feat_cols:\n",
    "        if f not in x_test_xgb.columns:\n",
    "            x_test_xgb[f] = 0.0\n",
    "            \n",
    "    xgb_scores = pred_xgb(xgb_models, x_test_xgb, xgb_feat_cols)\n",
    "\n",
    "\n",
    "    test_pairs = blend_predictions(\n",
    "        cat_scores, xgb_scores, original_submission, test_pairs,\n",
    "        w_cat=blend_weight_catboost, w_xgb=blend_weight_xgboost, w_orig=blend_weight_original\n",
    "    )\n",
    "\n",
    "    #разнообразие по авторам\n",
    "    def diversify_recs(df, books_df, max_per_author=3):\n",
    "        \"\"\"Ограничиваем книги одного автора для разнообразия\"\"\"\n",
    "        book_author = books_df.set_index(\"book_id\")[\"author_id\"].to_dict()\n",
    "        df[\"author_id\"] = df[\"book_id\"].map(book_author)\n",
    "        \n",
    "        diversified = []\n",
    "        for uid, group in tqdm(df.groupby(\"user_id\"), desc=\"Diversifying\"):\n",
    "            group = group.sort_values(\"score\", ascending=False)\n",
    "            author_count = {}\n",
    "            kept = []\n",
    "            \n",
    "            for _, row in group.iterrows():\n",
    "                aid = row[\"author_id\"]\n",
    "                if pd.isna(aid):\n",
    "                    kept.append(row)\n",
    "                    continue\n",
    "                author_count[aid] = author_count.get(aid, 0) + 1\n",
    "                if author_count[aid] <= max_per_author:\n",
    "                    kept.append(row)\n",
    "            \n",
    "            diversified.extend(kept[:20])\n",
    "        \n",
    "        return pd.DataFrame(diversified)\n",
    "    \n",
    "    #Применяем диверсификацию\n",
    "    test_pairs_div = diversify_recs(test_pairs, books)\n",
    "\n",
    "    #Финальный сабмишн\n",
    "    print(\"\\n=== building submission ===\")\n",
    "    ranked = test_pairs_div.sort_values([\"user_id\", \"score\"], ascending=[True, False])\n",
    "    \n",
    "    sub = ranked.groupby(\"user_id\")[\"book_id\"].apply(\n",
    "        lambda x: \",\".join(x.astype(int).astype(str).head(20))  \n",
    "    ).reset_index()\n",
    "    sub.columns = [\"user_id\", \"book_id_list\"]\n",
    "\n",
    "    final = targets.merge(sub, on=\"user_id\", how=\"left\")\n",
    "    final[\"book_id_list\"] = final[\"book_id_list\"].fillna(\"\")\n",
    "\n",
    "    # Статистика\n",
    "    rec_cnt = final[\"book_id_list\"].apply(lambda s: 0 if s == \"\" else len(str(s).split(\",\")))\n",
    "    print(f\"avg recs per user: {rec_cnt.mean():.2f}\")\n",
    "    print(f\"share users with 0 recs: {(rec_cnt == 0).mean():.4f}\")\n",
    "\n",
    "    # Сохранение\n",
    "    out_path = work_root / \"sub3ik.csv\"\n",
    "    final.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"\\nsubmission saved to: {out_path}\")\n",
    "    print(final.head())\n",
    "    print(\"#\" * 42)\n",
    "    print(\"pipeline finished\")\n",
    "    print(\"#\" * 42)\n",
    "    return final\n",
    "\n",
    "submission = main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8955990,
     "sourceId": 14070326,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
